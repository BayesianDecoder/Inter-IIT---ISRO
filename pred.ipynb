{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QU2Rc6CFBpev"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"modular_predict_abundancies_V1.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Nis05JW3w5ActCq6HQKEyBE2DnKsyHgv\n",
    "\"\"\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_al_path = \"../Desktop/Astral-Ray-Scratchpad-2/Soumik/ML/Trained_model/true_wt_al_model_final_new.pkl\"\n",
    "model_fe_path = \"../Desktop/Astral-Ray-Scratchpad-2/Soumik/ML/Trained_model/true_wt_fe_model_final_new.pkl\"\n",
    "model_mg_path = \"../Desktop/Astral-Ray-Scratchpad-2/Soumik/ML/Trained_model/true_wt_mg_model_final_new.pkl\"\n",
    "model_si_path = \"../Desktop/Astral-Ray-Scratchpad-2/Soumik/ML/Trained_model/true_wt_si_model_final_new.pkl\"\n",
    "\n",
    "json_path = \"../Desktop/ISRO.data_collection_v4.json\"\n",
    "\n",
    "# area_path='/content/drive/MyDrive/inter_IIT_ISRO/sampled-curve.txt'\n",
    "\n",
    "# output_path_csv=\"/content/predictions.csv\"\n",
    "\n",
    "# def classify_lunar_feature(latitude, longitude):\n",
    "#     # Primary classification: Highlands vs. Maria\n",
    "#     if (40 <= latitude <= 90 or -90 <= latitude <= -40) and -180 <= longitude <= 180:\n",
    "#         primary_class = \"Highlands\"\n",
    "#     elif -20 <= latitude <= 40 and -60 <= longitude <= 60:\n",
    "#         primary_class = \"Maria\"\n",
    "#     else:\n",
    "#         return \"Highlands - General\"\n",
    "\n",
    "#     if primary_class == \"Highlands\":\n",
    "#         if 40 <= latitude <= 60 and -40 <= longitude <= 10:\n",
    "#             return \"Highlands - Mountain Ranges\"\n",
    "#         elif 60 <= latitude <= 90 or -90 <= latitude <= -60:\n",
    "#             return \"Highlands - Craters\"\n",
    "#         elif 50 <= latitude <= 60 and -30 <= longitude <= -10:\n",
    "#             return \"Highlands - Valleys\"\n",
    "#         else:\n",
    "#             return \"Highlands - General\"\n",
    "#     elif primary_class == \"Maria\":\n",
    "#         if -10 <= latitude <= 40 and -60 <= longitude <= -20:\n",
    "#             return \"Maria - Lunar Domes\"\n",
    "#         elif -10 <= latitude <= 30 and -40 <= longitude <= 10:\n",
    "#             return \"Maria - Rilles\"\n",
    "#         else:\n",
    "#             return \"Maria - Basaltic Plains\"\n",
    "\n",
    "\n",
    "def classify_lunar_feature(latitude: float, longitude: float):\n",
    "    # Primary classification: Highlands vs. Maria\n",
    "    if (40 <= latitude <= 90 or -90 <= latitude <= -40) and -180 <= longitude <= 180:\n",
    "        primary_class = \"Highlands\"\n",
    "    elif -20 <= latitude <= 40 and -60 <= longitude <= 60:\n",
    "        primary_class = \"Maria\"\n",
    "    else:\n",
    "        return \"Highlands - General\"\n",
    "\n",
    "    if primary_class == \"Highlands\":\n",
    "        if 40 <= latitude <= 60 and -40 <= longitude <= 10:\n",
    "            return \"Highlands - Mountain Ranges\"\n",
    "        elif 60 <= latitude <= 90 or -90 <= latitude <= -60:\n",
    "            return \"Highlands - Craters\"\n",
    "        elif 50 <= latitude <= 60 and -30 <= longitude <= -10:\n",
    "            return \"Highlands - Valleys\"\n",
    "        else:\n",
    "            return \"Highlands - General\"\n",
    "    elif primary_class == \"Maria\":\n",
    "        if -10 <= latitude <= 40 and -60 <= longitude <= -20:\n",
    "            return \"Maria - Lunar Domes\"\n",
    "        elif -10 <= latitude <= 30 and -40 <= longitude <= 10:\n",
    "            return \"Maria - Rilles\"\n",
    "        else:\n",
    "            return \"Maria - Basaltic Plains\"\n",
    "\n",
    "\n",
    "def red_chi_2(element: str, df: pd.DataFrame):\n",
    "    df[f\"red_chi_2_{element}\"] = df[f\"chi_2_{element}\"] / (df[f\"dof_{element}\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_dict_column(df, column_name, prefix):\n",
    "    \"\"\"\n",
    "    Expand a dictionary column in a DataFrame into individual columns with a specified prefix.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column containing dictionaries.\n",
    "        prefix (str): The prefix to use for the new columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with new columns added.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists and contains dictionaries\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Check if the first non-null element is a dictionary\n",
    "    first_non_null = df[column_name].dropna().iloc[0]\n",
    "    if not isinstance(first_non_null, dict):\n",
    "        raise ValueError(f\"Column '{column_name}' does not contain dictionaries.\")\n",
    "\n",
    "    # Loop over the keys of the first dictionary\n",
    "    for key in first_non_null.keys():\n",
    "        new_column_name = f\"{prefix}{key}\"\n",
    "        df[new_column_name] = df[column_name].apply(lambda x: x.get(key) if pd.notnull(x) else None)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    \"model_al\": model_al_path,\n",
    "    \"model_fe\": model_fe_path,\n",
    "    \"model_mg\": model_mg_path,\n",
    "    \"model_si\": model_si_path,\n",
    "}\n",
    "\n",
    "# Define the feature sets for each model\n",
    "features = {\n",
    "    \"model_al\": [\n",
    "        \"wt_al\",\n",
    "        \"wt_si\",\n",
    "        \"photon_counts\",\n",
    "        \"solar_zenith_angle\",\n",
    "        \"emission_angle\",\n",
    "        \"altitude\",\n",
    "        \"exposure\",\n",
    "        \"peak_si_c\",\n",
    "        \"peak_mg_c\",\n",
    "        \"peak_al_c\",\n",
    "        \"peak_ti_c\",\n",
    "        \"peak_fe_c\",\n",
    "        \"peak_ca_c\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"Region_Highlands - Craters\",\n",
    "        \"Region_Highlands - Mountain Ranges\",\n",
    "        \"Region_Maria - Basaltic Plains\",\n",
    "        \"Region_Maria - Lunar Domes\",\n",
    "        \"Region_Maria - Rilles\",\n",
    "    ],\n",
    "    \"model_fe\": [\n",
    "        \"wt_mg\",\n",
    "        \"wt_al\",\n",
    "        \"wt_si\",\n",
    "        \"wt_fe\",\n",
    "        \"photon_counts\",\n",
    "        \"solar_zenith_angle\",\n",
    "        \"emission_angle\",\n",
    "        \"altitude\",\n",
    "        \"exposure\",\n",
    "        \"peak_si_c\",\n",
    "        \"peak_mg_c\",\n",
    "        \"peak_al_c\",\n",
    "        \"peak_ti_c\",\n",
    "        \"peak_fe_c\",\n",
    "        \"peak_ca_c\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"Region_Highlands - Craters\",\n",
    "        \"Region_Highlands - Mountain Ranges\",\n",
    "        \"Region_Maria - Basaltic Plains\",\n",
    "        \"Region_Maria - Lunar Domes\",\n",
    "        \"Region_Maria - Rilles\",\n",
    "    ],\n",
    "    \"model_mg\": [\n",
    "        \"wt_mg\",\n",
    "        \"wt_al\",\n",
    "        \"wt_si\",\n",
    "        \"photon_counts\",\n",
    "        \"solar_zenith_angle\",\n",
    "        \"emission_angle\",\n",
    "        \"altitude\",\n",
    "        \"exposure\",\n",
    "        \"peak_si_c\",\n",
    "        \"peak_mg_c\",\n",
    "        \"peak_al_c\",\n",
    "        \"peak_ti_c\",\n",
    "        \"peak_fe_c\",\n",
    "        \"peak_ca_c\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"Region_Highlands - Craters\",\n",
    "        \"Region_Highlands - Mountain Ranges\",\n",
    "        \"Region_Maria - Basaltic Plains\",\n",
    "        \"Region_Maria - Lunar Domes\",\n",
    "        \"Region_Maria - Rilles\",\n",
    "    ],\n",
    "    \"model_si\": [\n",
    "        \"wt_si\",\n",
    "        \"photon_counts\",\n",
    "        \"solar_zenith_angle\",\n",
    "        \"emission_angle\",\n",
    "        \"altitude\",\n",
    "        \"exposure\",\n",
    "        \"peak_si_c\",\n",
    "        \"peak_mg_c\",\n",
    "        \"peak_al_c\",\n",
    "        \"peak_ti_c\",\n",
    "        \"peak_fe_c\",\n",
    "        \"peak_ca_c\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"Region_Highlands - Craters\",\n",
    "        \"Region_Highlands - Mountain Ranges\",\n",
    "        \"Region_Maria - Basaltic Plains\",\n",
    "        \"Region_Maria - Lunar Domes\",\n",
    "        \"Region_Maria - Rilles\",\n",
    "        \"Region_Highlands - Valleys\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def one_hot_encode_region(df, column_name):\n",
    "    \"\"\"\n",
    "    One-hot encodes the specified column in a DataFrame for the given regions.\n",
    "    If the column is not present, raises an error.\n",
    "    If the column contains values not in the predefined list, ignores them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The column to one-hot encode.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoded columns added.\n",
    "    \"\"\"\n",
    "    # Predefined categories for one-hot encoding\n",
    "    allowed_categories = [\n",
    "        \"Highlands - Craters\",\n",
    "        \"Highlands - Mountain Ranges\",\n",
    "        \"Maria - Basaltic Plains\",\n",
    "        \"Maria - Lunar Domes\",\n",
    "        \"Maria - Rilles\",\n",
    "        \"Highlands - Valleys\",\n",
    "    ]\n",
    "\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"The column '{column_name}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # One-hot encoding logic\n",
    "    for category in allowed_categories:\n",
    "        encoded_column = f\"Region_{category}\"\n",
    "        df[encoded_column] = (df[column_name] == category).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def abun_pred(json_path, model_paths, features):\n",
    "    df = pd.read_json(json_path)\n",
    "\n",
    "    #   df=df.iloc[:10]\n",
    "\n",
    "    df = expand_dict_column(df, \"wt\", \"wt_\")\n",
    "    df = expand_dict_column(df, \"dof\", \"dof_\")\n",
    "    df = expand_dict_column(df, \"chi_2\", \"chi_2_\")\n",
    "    df = df = expand_dict_column(df, \"computed_metadata\", \"\")\n",
    "    # print(df.columns)\n",
    "    df[\"Region\"] = df.apply(lambda row: classify_lunar_feature(row[\"latitude\"], row[\"longitude\"]), axis=1)\n",
    "\n",
    "    df = red_chi_2(\"mg\", df)\n",
    "    df = red_chi_2(\"al\", df)\n",
    "    df = red_chi_2(\"si\", df)\n",
    "    df = red_chi_2(\"fe\", df)\n",
    "    df = red_chi_2(\"ti\", df)\n",
    "    df = red_chi_2(\"ca\", df)\n",
    "\n",
    "    data = one_hot_encode_region(df, \"Region\")\n",
    "\n",
    "    data[\"emission_angle\"] = 90 - data[\"emission_angle\"]\n",
    "    data[\"solar_zenith_angle\"] = 90 - data[\"solar_zenith_angle\"]\n",
    "\n",
    "    data[\"lat\"] = data[\"latitude\"]\n",
    "    data[\"long\"] = data[\"longitude\"]\n",
    "    # data=data.drop(columns=['Region','Region_Highlands - General'])\n",
    "    # print(data.columns)\n",
    "    # print(data.head())\n",
    "    predictions = {}\n",
    "\n",
    "    for model_name, model_path in model_paths.items():\n",
    "        # Check if the model file exists\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file for {model_name} not found at path: {model_path}\")\n",
    "\n",
    "        # Load the model using pickle\n",
    "        with open(model_path, \"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "        # Retrieve the feature set for the current model\n",
    "        feature_set = features[model_name]\n",
    "\n",
    "        # Check if all required features are present in the data\n",
    "        missing_features = [feature for feature in feature_set if feature not in data.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features for {model_name}: {missing_features}\")\n",
    "\n",
    "        # Prepare the input data for prediction\n",
    "        model_input = data[feature_set]\n",
    "\n",
    "        # Make predictions\n",
    "        preds = model.predict(model_input)\n",
    "\n",
    "        # Store the predictions with a descriptive column name\n",
    "        prediction_column = f\"{model_name}_prediction\"\n",
    "        predictions[prediction_column] = preds\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    combined_df = pd.concat([data[[\"latitude\", \"longitude\", \"wt_fe\", \"wt_al\", \"wt_mg\", \"wt_si\"]], predictions_df], axis=1)\n",
    "    # print(combined_df.columns)\n",
    "    combined_df_json = combined_df.to_json(orient=\"records\")\n",
    "    return combined_df_json\n",
    "    # combined_df.to_csv(output_path)\n",
    "\n",
    "\n",
    "print(abun_pred(json_path, model_paths, features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          2.28571429  3.57142857  4.85714286  6.14285714  7.42857143\n",
      "  8.71428571 10.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate an array of 10 points between 1 and 10\n",
    "arr = np.linspace(1, 10, num=8)\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
